{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "from performance_test import *\n",
    "from kafka_producer import *\n",
    "from directNeo4jImporter import *\n",
    "from node_deletion import *\n",
    "from correctness_test import *\n",
    "from partition_changer import *\n",
    "from heapSize_changer import *\n",
    "from streams_conf_changer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter\n",
    "\n",
    "#### General Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_path = '../settings.json'\n",
    "with open(settings_path) as json_file:\n",
    "    settings = json.load(json_file)\n",
    "settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## put in settings(?)\n",
    "#name of database\n",
    "\n",
    "node_labels = {\n",
    "    'original': {\n",
    "        'block'       : settings['node_labels_neo4j']['block'],\n",
    "        'transaction' : settings['node_labels_neo4j']['transaction'],\n",
    "        'address'     : settings['node_labels_neo4j']['address']\n",
    "    },\n",
    "    'test': {\n",
    "        'block'       : 'Block_test',\n",
    "        'transaction' : 'Transaction_test',\n",
    "        'address'     : 'Address_test'\n",
    "    }\n",
    "}\n",
    "\n",
    "neo4j_port = str(7687)\n",
    "\n",
    "\n",
    "path_to_neo4j_conf_directory = settings['path_to_neo4j_conf_dir']\n",
    "path_to_streams_conf = path_to_neo4j_conf_directory + '/streams.conf'\n",
    "path_to_neo4j_conf = path_to_neo4j_conf_directory + '/neo4j.conf'\n",
    "\n",
    "kafka_topics =   {\n",
    "    'transaction': settings['kafka_topics']['transaction'],\n",
    "    'block': settings['kafka_topics']['block']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Performance Parameters\n",
    "\n",
    "# set to false if test_nodes are inserted \n",
    "evaluate_original = True\n",
    "evaluate_test     = True\n",
    "\n",
    "nodeTypesToTest = []\n",
    "if evaluate_test == True:\n",
    "    nodeTypesToTest.append(False)\n",
    "if evaluate_original == True:\n",
    "    nodeTypesToTest.append(True)\n",
    "\n",
    "\n",
    "# check correctness of nodes inserted\n",
    "check_correctness = False\n",
    "\n",
    "# deleting Nodes at the end of each run\n",
    "deleteNodes = True\n",
    "\n",
    "# nr of runs per set of parameter configurations\n",
    "subRuns = 5\n",
    "\n",
    "#################### block heights\n",
    "    \n",
    "blockRangeToInsert = {\n",
    "    'original': {\n",
    "        'start_block_height' : 591116,\n",
    "        'end_block_height'   : 591126\n",
    "    }, \n",
    "    'test': {\n",
    "        'start_block_height' : 1,\n",
    "        'end_block_height'   : 10001\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# whether nodes to delete were already collected and saved\n",
    "originalNodesCached = False    \n",
    "\n",
    "nodesToDeleteFilePath = \"deletion_nodes_591116_10.json\"\n",
    "\n",
    "# skipping Matching on previous transaction for original nodes if set to True\n",
    "deactivateMatchOnPreviousOnOriginal = False\n",
    "    \n",
    "############## Evaluation Parameters\n",
    "\n",
    "# number of kafka partitions \n",
    "kafka_partitions_list = [1,2,4,8]\n",
    "\n",
    "# batch processing - MISSING implementation (!)\n",
    "#batching_list = [1, 100, 400, 800]\n",
    "batching_list = [0, 1, 10, 100]\n",
    "\n",
    "# neo4j heap size\n",
    "heap_size_list = [5, 8, 16]\n",
    "\n",
    "# matching input on previous address\n",
    "match_on_previous_add = True  #(?) => previous address is matched in the script (bitcoind)\n",
    "\n",
    "# bypass kafka pipeline\n",
    "bypass_kafka = False \n",
    "\n",
    "\n",
    "\n",
    "############## Default testing parameter values\n",
    "\n",
    "default_kafka_partitions = kafka_partitions_list[0]\n",
    "default_batching_size = batching_list[0]\n",
    "default_heap_size = heap_size_list[0]\n",
    "default_match_on_previous_add = True\n",
    "default_bypass_kafka = False\n",
    "\n",
    "\n",
    "\n",
    "############## Booleans whether or not to test on parameter\n",
    "testKafkaPartitions = True\n",
    "testBatching        = True\n",
    "testHeapSize        = True\n",
    "testMatchOnPrevAddr = True\n",
    "testBypassKafka     = True\n",
    "\n",
    "nodeTypesToTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Parameter Configurations list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Parameter configurations [kafka_partitions, batching_size, heap_size, match_on_previous_add, bypass_kafka]\n",
    "\n",
    "configurations= [[default_kafka_partitions, \n",
    "                  default_batching_size, \n",
    "                  default_heap_size, \n",
    "                  default_match_on_previous_add, \n",
    "                  default_bypass_kafka]]\n",
    "\n",
    "# change kafka_partitions parameter\n",
    "if testKafkaPartitions == True:\n",
    "    for i in kafka_partitions_list:\n",
    "        if i == default_kafka_partitions:\n",
    "            continue\n",
    "        else: \n",
    "            add = [i, default_batching_size, default_heap_size, default_match_on_previous_add, default_bypass_kafka]\n",
    "            configurations.append(add)\n",
    "\n",
    "# change batching_size parameter\n",
    "if testBatching == True:\n",
    "    for i in batching_list:\n",
    "        if i == default_batching_size:\n",
    "            continue\n",
    "        else: \n",
    "            add = [default_kafka_partitions, i, default_heap_size, default_match_on_previous_add, default_bypass_kafka]\n",
    "            configurations.append(add)\n",
    "\n",
    "# change heap_size parameter    \n",
    "if testHeapSize == True:\n",
    "    for i in heap_size_list:\n",
    "        if i == default_heap_size:\n",
    "            continue\n",
    "        else: \n",
    "            add = [default_kafka_partitions, default_batching_size, i, default_match_on_previous_add, default_bypass_kafka]\n",
    "            configurations.append(add)\n",
    "\n",
    "# change match_on_previous_add parameter\n",
    "if testMatchOnPrevAddr == True:\n",
    "    change_match_on_previous_add = not default_match_on_previous_add\n",
    "    add = [default_kafka_partitions, default_batching_size, default_heap_size, change_match_on_previous_add, default_bypass_kafka]\n",
    "    configurations.append(add)\n",
    "\n",
    "# change bypass_kafka parameter\n",
    "if testBypassKafka == True:\n",
    "    change_bypass_kafka = not default_bypass_kafka\n",
    "    add = [default_kafka_partitions, default_batching_size, default_heap_size, default_match_on_previous_add, change_bypass_kafka]\n",
    "    configurations.append(add)\n",
    "\n",
    "display('[kafka_partitions, batching_size, heap_size, match_on_previous_add, bypass_kafka]')\n",
    "configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Results dataframe\n",
    "columns=[\n",
    "    'experimentRun',\n",
    "    'subRun',\n",
    "    'evaluate_original',\n",
    "    'start_block_height',\n",
    "    'end_block_height',\n",
    "    'kafka_partitions',\n",
    "    'batching',\n",
    "    'match_on_previous_add',\n",
    "    'heap_size',\n",
    "    'bypass_kafka',\n",
    "    'check_correctness',\n",
    "    'endTimePart1', \n",
    "    'endTimePart2', \n",
    "    'totalExecutionTime', \n",
    "    'timeOutReached']\n",
    "\n",
    "results_df = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory structure\n",
    "result_dir     = 'results'\n",
    "mismatches_dir = 'mismatches'\n",
    "if os.path.exists(result_dir) == False:\n",
    "    os.makedirs(result_dir)\n",
    "if os.path.exists(os.path.join(result_dir, mismatches_dir)) == False:\n",
    "    os.makedirs(os.path.join(result_dir, mismatches_dir))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# collect original nodes that need to be deleted in case of inserting data to original topics\n",
    "#if evaluate_original == True and originalNodesCached == False:\n",
    "\n",
    "\n",
    "if originalNodesCached == False:\n",
    "    \n",
    "    # collect nodes of original database\n",
    "    noderange = 'original'\n",
    "    \n",
    "    # ToDo: check correctness of starting hight\n",
    "    # #query neo4j if \n",
    "\n",
    "    # collecting nodes for \n",
    "    deletion_nodes = getDeletionList(start_block_height = blockRangeToInsert[noderange]['start_block_height'], \n",
    "                                     end_block_height = blockRangeToInsert[noderange]['end_block_height'], \n",
    "                                     label_address = node_labels['original']['address'], \n",
    "                                     neo4j_location = 'server', \n",
    "                                     neo4j_port = '7687')\n",
    "    \n",
    "    import json\n",
    "    # save deletion nodes to json file\n",
    "\n",
    "    with open(nodesToDeleteFilePath, \"w\") as outfile: \n",
    "        json.dump(deletion_nodes, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(nodesToDeleteFilePath) as infile:\n",
    "    deletion_nodes = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "EvaluationStartTime=time.time()\n",
    "# for each set of settings (for each experiment run) make counter\n",
    "# -> experimentRun\n",
    "# (hand over to mismachtches)\n",
    "\n",
    "\n",
    "## loop for evaluate original or test nodes\n",
    "for boolean in nodeTypesToTest:\n",
    "    evaluate_original = boolean\n",
    "\n",
    "    experimentRun = 0\n",
    "\n",
    "    if evaluate_original == True:\n",
    "        noderange = 'original'\n",
    "    else:\n",
    "        noderange = 'test'\n",
    "    start_block_height = blockRangeToInsert[noderange]['start_block_height']\n",
    "    end_block_height   = blockRangeToInsert[noderange]['end_block_height']\n",
    "\n",
    "\n",
    "    for i in range(len(configurations)): \n",
    "\n",
    "        j = 0 \n",
    "        experimentRun = experimentRun + 1\n",
    "        kafka_partitions = configurations[i][j]\n",
    "        batching_size = configurations[i][j+1]\n",
    "        heap_size = configurations[i][j+2]\n",
    "        match_on_previous_add = configurations[i][j+3]\n",
    "        bypass_kafka = configurations[i][j+4]\n",
    "        \n",
    "        # check whether to skip Matching on previous Transaction on original nodes\n",
    "        if deactivateMatchOnPreviousOnOriginal == True and \\\n",
    "            match_on_previous_add == False             and \\\n",
    "            evaluate_original == True:\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            print('Experiment run: ', experimentRun)\n",
    "            #print(kafka_partitions, batching_size, heap_size, match_on_previous_add, bypass_kafka)\n",
    "\n",
    "\n",
    "            # change cypher templates - match_on_previous_add can change\n",
    "            changeStreamsFile(path = path_to_streams_conf, \n",
    "                              kafka_topics = kafka_topics, \n",
    "                              evaluate_original = evaluate_original, \n",
    "                              matchOnAddress = match_on_previous_add, \n",
    "                              getTemplate = True,    # to retrieve cypher template or query for direct insertion \n",
    "                              node_labels = node_labels, \n",
    "                              evaluation = True)\n",
    "\n",
    "\n",
    "            # check & change batching parameter\n",
    "            with open(path_to_streams_conf, 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    if 'kafka.fetch.min.bytes' in line:\n",
    "                        sizeString = line.split('=')[1]\n",
    "                        currentBatching_minbytes = int(re.findall(r'\\d+', sizeString)[0])\n",
    "\n",
    "\n",
    "            changeBatching = True\n",
    "            # default waittime\n",
    "            batching_waittime = 500\n",
    "            # default min bytes \n",
    "            batching_minbytes =   1\n",
    "\n",
    "            if (batching_size == default_batching_size and currentBatching_minbytes == 1) or (batching_size != default_batching_size and currentBatching_minbytes/550 == batching_size):\n",
    "                changeBatching = False\n",
    "\n",
    "            elif batching_size == default_batching_size and currentBatching_minbytes != 1: \n",
    "                # default waittime\n",
    "                batching_waittime = 500\n",
    "\n",
    "                # default min bytes \n",
    "                batching_minbytes =   1\n",
    "\n",
    "            elif batching_size != default_batching_size and currentBatching_minbytes/550 != batching_size:\n",
    "\n",
    "                # 0.1 second * batch size\n",
    "                batching_waittime = 100 * batching_size \n",
    "\n",
    "                # bytes per object (transaction) * batch size\n",
    "                batching_minbytes = 555 * batching_size\n",
    "\n",
    "            if changeBatching == True:\n",
    "                # change streams.conf file \n",
    "                os.popen(f'''sed -i '/kafka.fetch.max.wait.ms/c\\kafka.fetch.max.wait.ms={batching_waittime}' /etc/neo4j/streams.conf''')\n",
    "                os.popen(f'''sed -i '/kafka.fetch.min.bytes/c\\kafka.fetch.min.bytes={batching_minbytes}'     /etc/neo4j/streams.conf''')\n",
    "\n",
    "\n",
    "            time.sleep(5)\n",
    "            # check & change batching parameter\n",
    "            with open(path_to_streams_conf, 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    if 'kafka.fetch.min.bytes' in line:\n",
    "                        sizeString = line.split('=')[1]\n",
    "                        currentBatching_minbytes = int(re.findall(r'\\d+', sizeString)[0])\n",
    "\n",
    "\n",
    "            # Parameters that require to start neo4j\n",
    "\n",
    "            #changePartitions = False\n",
    "            #changeHeapSize   = False\n",
    "\n",
    "\n",
    "            # check kafka partitions\n",
    "            # check the value of changing parameters and process accordingly\n",
    "            stream = os.popen(f\"/home/kafka/kafka/bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic {kafka_topics['block']}\")\n",
    "            output = stream.read()\n",
    "            partitionCountPhrase = 'PartitionCount: ' \n",
    "            numberOfCurrentPartitionsString = output[(output.find(partitionCountPhrase)+len(partitionCountPhrase)):].split('\\t')[0]\n",
    "            try:\n",
    "                numberOfCurrentPartitions = int(numberOfCurrentPartitionsString)\n",
    "            except:\n",
    "                partitionChanger(kafka_partitions)\n",
    "\n",
    "\n",
    "            if numberOfCurrentPartitions != default_kafka_partitions:\n",
    "                partitionChanger(kafka_partitions)\n",
    "\n",
    "\n",
    "\n",
    "            # check heap size\n",
    "            with open(path_to_neo4j_conf, 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    if 'dbms.memory.heap.initial_size' in line:\n",
    "                        sizeString = line.split('=')[1]\n",
    "                        initialHeapSize = int(re.findall(r'\\d+', sizeString)[0])\n",
    "                    if 'dbms.memory.heap.max_size' in line:\n",
    "                        sizeString = line.split('=')[1]\n",
    "                        maxHeapSize = int(re.findall(r'\\d+', sizeString)[0])\n",
    "\n",
    "            if initialHeapSize != maxHeapSize or initialHeapSize != default_heap_size:\n",
    "                heapSizeChanger(heap_size)\n",
    "\n",
    "\n",
    "\n",
    "            #start loop for rerunning each configuration steps\n",
    "            for subRun in range(subRuns):\n",
    "                subRun += 1\n",
    "\n",
    "                # insertion and performance testing\n",
    "                endTimePart1, endTimePart2, totalExecutionTime, timeOutReached = runPerformanceTest(evaluate_original, \n",
    "                                                                                                node_labels, \n",
    "                                                                                                bypass_kafka, \n",
    "                                                                                                start_block_height, \n",
    "                                                                                                end_block_height, \n",
    "                                                                                                match_on_previous_add, \n",
    "                                                                                                kafka_topics)   \n",
    "\n",
    "                # correctness testing\n",
    "                if evaluate_original == False and check_correctness == True:\n",
    "                    checkCorrectness(start_block_height, \n",
    "                                     end_block_height,\n",
    "                                     node_labels,\n",
    "                                     neo4j_port,\n",
    "                                     experimentRun, \n",
    "                                     printMismatches=False, \n",
    "                                     saveMismatches=True)\n",
    "\n",
    "                # result collection\n",
    "                data = [[\n",
    "                    experimentRun,\n",
    "                    subRun,\n",
    "                    evaluate_original,\n",
    "                    start_block_height,\n",
    "                    end_block_height,\n",
    "                    kafka_partitions,\n",
    "                    batching_size,\n",
    "                    match_on_previous_add,\n",
    "                    heap_size,\n",
    "                    bypass_kafka,\n",
    "                    check_correctness,\n",
    "                    endTimePart1, \n",
    "                    endTimePart2, \n",
    "                    totalExecutionTime, \n",
    "                    timeOutReached]]\n",
    "\n",
    "                new_results_entry = pd.DataFrame(columns=columns, data = data)\n",
    "                results_df=pd.concat([results_df,new_results_entry]).sort_index()\n",
    "\n",
    "                # writing results to csv\n",
    "                results_df.to_csv('./results/evaluation_results.csv', index=False)\n",
    "\n",
    "                time.sleep((end_block_height-start_block_height)*0.001)\n",
    "\n",
    "                # deletion of inserted nodes\n",
    "                if deleteNodes == True:\n",
    "                    if evaluate_original == True:\n",
    "                        deleteOriginalEvaluationNodes(deletion_nodes = deletion_nodes, \n",
    "                                                      node_labels = node_labels,\n",
    "                                                      neo4j_location = 'server', \n",
    "                                                      neo4j_port = '7687')\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        deleteTestEvaluationNodes(node_labels = node_labels,\n",
    "                                                  neo4j_location = 'server', \n",
    "                                                  neo4j_port = '7687')\n",
    "\n",
    "\n",
    "                display(results_df)\n",
    "                print(f'Run {experimentRun} finished \\n')\n",
    "\n",
    "            \n",
    "\n",
    "#display(results_df) \n",
    "    \n",
    "display(results_df)\n",
    "EvaluationTime=time.time() - EvaluationStartTime\n",
    "print('Evaluation Time: ', np.round(EvaluationTime/60, 2), ' min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After loop - Evaluation Process cleanup\n",
    "\n",
    "Restore streams file\n",
    "- remove insertion time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('./results/evaluation_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore streams file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changeStreamsFile(path = path_to_streams_conf, \n",
    "                  kafka_topics = kafka_topics, \n",
    "                  evaluate_original = True, \n",
    "                  matchOnAddress = True, \n",
    "                  getTemplate = True,  \n",
    "                  node_labels = node_labels, \n",
    "                  evaluation = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
